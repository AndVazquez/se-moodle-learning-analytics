{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6c74460-5000-4565-94e1-52afde874d48",
   "metadata": {},
   "source": [
    "# A1: Data Pseudonymization and Processing Pipeline\n",
    "\n",
    "**Purpose:** This notebook implements a reproducible pipeline for preparing multi-year Moodle learning management system (LMS) logs for educational data mining analysis. It merges activity logs with student evaluation data while preserving participant anonymity through cryptographic pseudonymization.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "1. **Data Loading**: Import Moodle activity logs and evaluation spreadsheets\n",
    "2. **Temporal Filtering**: Optionally restrict logs to a specific academic period\n",
    "3. **Name Matching**: Link records across datasets using exact and fuzzy matching\n",
    "4. **Data Merging**: Combine activity and evaluation data at the record level\n",
    "5. **Pseudonymization**: Replace identifiable information with salted cryptographic hashes\n",
    "6. **Export**: Generate analysis-ready, privacy-preserving datasets\n",
    "\n",
    "## Privacy Safeguards\n",
    "\n",
    "- Student identities are replaced with salted SHA-256 hashes (non-reversible)\n",
    "- Teachers receive deterministic labels without exposing real names\n",
    "- Sensitive columns (IP addresses, raw names) are dropped before export\n",
    "- The salt value is never logged or saved to output files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-section",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "The following cell imports all required libraries:\n",
    "\n",
    "| Library | Purpose |\n",
    "|---------|--------|\n",
    "| `pandas` | Data manipulation and CSV I/O |\n",
    "| `hashlib` | SHA-256 hashing for pseudonymization |\n",
    "| `datetime` | Date parsing and temporal filtering |\n",
    "| `difflib.SequenceMatcher` | String similarity computation |\n",
    "| `re` | Regular expression-based text cleaning |\n",
    "| `unicodedata` | Unicode normalization (accent removal) |\n",
    "| `collections.Counter` | Tracking filtered records |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26a650d4-6aad-453d-8dd7-8789524fde13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc640ea4-8f6e-41e5-9cbe-d777a1dcb95e",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n",
    "\n",
    "This section defines reusable utility functions organized into three categories:\n",
    "\n",
    "### 2.1 Name Normalization\n",
    "Functions for cleaning and standardizing names to enable robust matching across datasets with inconsistent formatting.\n",
    "\n",
    "### 2.2 Name Matching\n",
    "Functions implementing the similarity-based matching algorithm that combines sequence alignment with set-based (Jaccard) comparison.\n",
    "\n",
    "### 2.3 Data Transformation\n",
    "Functions for pseudonymization, date parsing, and attendance calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "name-norm-section",
   "metadata": {},
   "source": [
    "### 2.1 Name Normalization Functions\n",
    "\n",
    "These functions handle the variability commonly found in name data:\n",
    "- Diacritical marks (accents) that may be present or absent\n",
    "- Inconsistent capitalization\n",
    "- Extra whitespace\n",
    "- Name particles (e.g., \"de\", \"la\") that add noise to matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "name-normalization-funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove diacritical marks while preserving base characters.\n",
    "    \n",
    "    Uses Unicode NFKD normalization to decompose characters into base\n",
    "    characters and combining marks, then removes the combining marks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Input string potentially containing accented characters.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        String with diacritical marks removed (e.g., 'GARCÍA' -> 'GARCIA').\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> strip_accents('José María')\n",
    "    'Jose Maria'\n",
    "    \"\"\"\n",
    "    normalized = unicodedata.normalize('NFKD', text)\n",
    "    return ''.join(char for char in normalized if not unicodedata.combining(char))\n",
    "\n",
    "\n",
    "def clean_name(name) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a name for comparison: trim, uppercase, collapse spaces, remove accents.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str or None\n",
    "        Raw name string from input data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Cleaned, uppercase name with single spaces and no accents.\n",
    "        Returns empty string for null/NaN input.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> clean_name('  María   García  ')\n",
    "    'MARIA GARCIA'\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    cleaned = str(name).strip().upper()\n",
    "    cleaned = strip_accents(cleaned)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def normalize_name_parts(name) -> set:\n",
    "    \"\"\"\n",
    "    Extract meaningful name tokens as an unordered set.\n",
    "    \n",
    "    Removes common Spanish name particles and single-character tokens\n",
    "    to focus on substantive name components. The set representation\n",
    "    allows order-independent comparison.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str or None\n",
    "        Input name string.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    set\n",
    "        Set of normalized name tokens (uppercase, no accents).\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> normalize_name_parts('María de la Cruz García')\n",
    "    {'MARIA', 'CRUZ', 'GARCIA'}\n",
    "    \"\"\"\n",
    "    if pd.isna(name) or str(name).strip() == \"\":\n",
    "        return set()\n",
    "    \n",
    "    cleaned = clean_name(name)\n",
    "    # Common Spanish name particles to exclude\n",
    "    particles = {'DE', 'LA', 'LAS', 'DEL', 'LOS', 'Y'}\n",
    "    words = [w for w in cleaned.split() if w not in particles and len(w) > 1]\n",
    "    return set(words)\n",
    "\n",
    "\n",
    "def is_system_account(name) -> bool:\n",
    "    \"\"\"\n",
    "    Identify non-student accounts (administrators, guests, system users).\n",
    "    \n",
    "    These accounts should be excluded from student-focused analyses\n",
    "    and from the name matching process.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str or None\n",
    "        Account name to check.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the name indicates a system/admin account.\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return False\n",
    "    \n",
    "    name_clean = clean_name(name)\n",
    "    system_indicators = [\n",
    "        'ADMINISTRADOR', 'ADMIN', 'SYSTEM', 'STUDIUM',\n",
    "        'USUARIO', 'USER', 'INVITADO', 'GUEST'\n",
    "    ]\n",
    "    \n",
    "    if name_clean == '-':\n",
    "        return True\n",
    "    if name_clean in system_indicators:\n",
    "        return True\n",
    "    if any(indicator in name_clean for indicator in system_indicators):\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matching-section",
   "metadata": {},
   "source": [
    "### 2.2 Name Matching Functions\n",
    "\n",
    "The matching algorithm uses a hybrid similarity measure that combines:\n",
    "\n",
    "1. **Sequence Similarity (30% weight)**: Uses `difflib.SequenceMatcher` to measure character-level alignment. Good for catching typos and minor variations.\n",
    "\n",
    "2. **Jaccard Similarity (70% weight)**: Compares the set of name tokens between two names. This is robust to word order differences (e.g., \"García López\" vs \"López García\").\n",
    "\n",
    "The higher weight on Jaccard similarity reflects the common occurrence of name component reordering in our data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "name-matching-funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_name_similarity(name1, name2) -> float:\n",
    "    \"\"\"\n",
    "    Compute hybrid similarity score between two names.\n",
    "    \n",
    "    Combines character-level sequence similarity (30%) with token-level\n",
    "    Jaccard similarity (70%) to handle both typos and word reordering.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name1 : str\n",
    "        First name to compare.\n",
    "    name2 : str\n",
    "        Second name to compare.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Similarity score in range [0.0, 1.0].\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> calculate_name_similarity('Juan García López', 'Juan López García')\n",
    "    0.85  # High score despite word reordering\n",
    "    \"\"\"\n",
    "    # Character-level sequence similarity\n",
    "    clean1, clean2 = clean_name(name1), clean_name(name2)\n",
    "    sequence_sim = SequenceMatcher(None, clean1, clean2).ratio()\n",
    "    \n",
    "    # Token-level Jaccard similarity\n",
    "    tokens1 = normalize_name_parts(name1)\n",
    "    tokens2 = normalize_name_parts(name2)\n",
    "    \n",
    "    if not tokens1 or not tokens2:\n",
    "        return sequence_sim\n",
    "    \n",
    "    intersection = len(tokens1 & tokens2)\n",
    "    union = len(tokens1 | tokens2)\n",
    "    jaccard_sim = intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    # Weighted combination: favor token matching for robustness to reordering\n",
    "    return 0.3 * sequence_sim + 0.7 * jaccard_sim\n",
    "\n",
    "\n",
    "def find_best_match(target_name, candidate_names, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Find the best matching name from a list of candidates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    target_name : str\n",
    "        Name to find a match for.\n",
    "    candidate_names : list of str\n",
    "        List of potential matching names.\n",
    "    threshold : float, default=0.6\n",
    "        Minimum similarity score required for a valid match.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple (str or None, float)\n",
    "        Best matching name and its similarity score.\n",
    "        Returns (None, 0.0) if no match exceeds the threshold.\n",
    "    \"\"\"\n",
    "    if is_system_account(target_name):\n",
    "        return None, 0.0\n",
    "    \n",
    "    best_match = None\n",
    "    best_score = 0.0\n",
    "    \n",
    "    for candidate in candidate_names:\n",
    "        if is_system_account(candidate):\n",
    "            continue\n",
    "        \n",
    "        score = calculate_name_similarity(target_name, candidate)\n",
    "        if score >= threshold and score > best_score:\n",
    "            best_match = candidate\n",
    "            best_score = score\n",
    "    \n",
    "    return best_match, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transform-section",
   "metadata": {},
   "source": [
    "### 2.3 Data Transformation Functions\n",
    "\n",
    "These functions handle:\n",
    "\n",
    "- **Pseudonymization**: Generating non-reversible identifiers using salted SHA-256 hashing\n",
    "- **Date Parsing**: Converting Moodle's date format strings to datetime objects\n",
    "- **Attendance Calculation**: Parsing various attendance formats into percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "transformation-funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pseudonym(name, salt=None) -> str:\n",
    "    \"\"\"\n",
    "    Generate a pseudonym using salted SHA-256 hashing.\n",
    "    \n",
    "    The salt prevents dictionary-based re-identification attacks.\n",
    "    The same name with the same salt always produces the same pseudonym,\n",
    "    enabling consistent identification across datasets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        Original name to pseudonymize.\n",
    "    salt : str, optional\n",
    "        Secret salt value. Strongly recommended for privacy protection.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Pseudonym in format 'STU_XXXXXXXX' (8 hex characters).\n",
    "    \"\"\"\n",
    "    base = clean_name(name)\n",
    "    key = (salt or \"\") + \"|\" + base\n",
    "    hash_value = hashlib.sha256(key.encode(\"utf-8\")).hexdigest()[:8].upper()\n",
    "    return f\"STU_{hash_value}\"\n",
    "\n",
    "\n",
    "def parse_date(date_str):\n",
    "    \"\"\"\n",
    "    Parse Moodle date format strings to datetime objects.\n",
    "    \n",
    "    Supports formats:\n",
    "    - 'DD/MM/YYYY'\n",
    "    - 'DD/MM/YYYY HH:MM'\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    date_str : str or None\n",
    "        Date string from Moodle logs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    datetime or None\n",
    "        Parsed datetime object, or None if parsing fails.\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    \n",
    "    date_string = str(date_str).strip()\n",
    "    \n",
    "    # Try date-only format first\n",
    "    try:\n",
    "        date_part = date_string.split()[0]\n",
    "        return datetime.strptime(date_part, \"%d/%m/%Y\")\n",
    "    except (ValueError, IndexError):\n",
    "        pass\n",
    "    \n",
    "    # Try full datetime format\n",
    "    try:\n",
    "        return datetime.strptime(date_string, \"%d/%m/%Y %H:%M\")\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_attendance(value, total_sessions) -> float:\n",
    "    \"\"\"\n",
    "    Parse attendance data into a percentage.\n",
    "    \n",
    "    Handles multiple input formats:\n",
    "    - Integer count (e.g., 24 sessions attended)\n",
    "    - Fraction string (e.g., '24 de 30' or '24/30')\n",
    "    - Pre-computed percentage\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    value : int, float, or str\n",
    "        Attendance value in any supported format.\n",
    "    total_sessions : int\n",
    "        Total number of sessions in the course.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Attendance percentage in range [0, 100], or NaN if not computable.\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return float('nan')\n",
    "    \n",
    "    # Handle numeric values\n",
    "    if isinstance(value, (int, float)):\n",
    "        # If value looks like a count (<= total_sessions), convert to percentage\n",
    "        if total_sessions and value <= total_sessions:\n",
    "            return (value / total_sessions) * 100.0\n",
    "        # Otherwise assume it's already a percentage\n",
    "        return float(value)\n",
    "    \n",
    "    # Handle string values\n",
    "    text = str(value)\n",
    "    numbers = [int(x) for x in re.findall(r'\\d+', text)]\n",
    "    \n",
    "    if len(numbers) == 0:\n",
    "        return float('nan')\n",
    "    \n",
    "    if len(numbers) == 1:\n",
    "        # Single number: treat as attended count\n",
    "        attended = numbers[0]\n",
    "        if total_sessions:\n",
    "            return (attended / total_sessions) * 100.0\n",
    "        return float('nan')\n",
    "    \n",
    "    # Two numbers: assume format is \"attended / total\"\n",
    "    attended, total = numbers[0], numbers[1]\n",
    "    if total == 0:\n",
    "        return float('nan')\n",
    "    return (attended / total) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de95a90-36fb-401a-81b7-87f7dbecb8ce",
   "metadata": {},
   "source": [
    "## 3. Main Processing Function\n",
    "\n",
    "The `merge_moodle_evaluation_data` function implements the complete data processing pipeline. It performs the following steps:\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 1 | Load Moodle logs and evaluation data from CSV files |\n",
    "| 2 | Validate required columns exist in both datasets |\n",
    "| 3 | Apply temporal filter to restrict logs to the academic period |\n",
    "| 4 | Construct normalized full names for matching |\n",
    "| 5 | Match names in three phases: exact → fuzzy → manual overrides |\n",
    "| 6 | Filter out unmatched records (non-students, non-teachers) |\n",
    "| 7 | Merge evaluation fields onto Moodle log records |\n",
    "| 8 | Pseudonymize identities (deterministic teacher labels, salted student hashes) |\n",
    "| 9 | Drop sensitive columns and compute derived fields |\n",
    "\n",
    "The function returns both the processed DataFrame and the name-to-pseudonym mapping (kept in memory only, never written to disk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f460100e-e122-474b-b49d-7246dbbf83df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Helper to mask names in console logs (not used in the actual data)\n",
    "# -------------------------------------------------------------------------\n",
    "def make_log_name_masker():\n",
    "    \"\"\"\n",
    "    Returns a function that gives each real name a stable alias\n",
    "    for console display only. Aliases are not stored or written to disk.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    \n",
    "    def mask(name):\n",
    "        if name is None:\n",
    "            return \"<NULL>\"\n",
    "        if name not in mapping:\n",
    "            mapping[name] = f\"ALIAS_{len(mapping) + 1}\"\n",
    "        return mapping[name]\n",
    "    \n",
    "    return mask\n",
    "    \n",
    "def merge_moodle_evaluation_data(\n",
    "    moodle_file,\n",
    "    evaluation_file,\n",
    "    total_sessions=30,\n",
    "    limit_date=None,\n",
    "    teachers=None,\n",
    "    output_file=None,\n",
    "    manual_matches=None,\n",
    "    fuzzy_threshold=0.6,\n",
    "    suggestion_threshold=0.5,\n",
    "    salt=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Merge Moodle logs with evaluation data and pseudonymize identities.\n",
    "    \n",
    "    This function implements a complete ETL pipeline for preparing\n",
    "    educational data mining datasets while preserving participant privacy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    moodle_file : str\n",
    "        Path to Moodle activity logs CSV file.\n",
    "    evaluation_file : str\n",
    "        Path to student evaluation data CSV file.\n",
    "    total_sessions : int, default=30\n",
    "        Total number of class sessions (for attendance percentage calculation).\n",
    "    limit_date : str, optional\n",
    "        Cutoff date in 'DD/MM/YYYY' format. Moodle actions after this\n",
    "        date are excluded from the output.\n",
    "    teachers : list of str, optional\n",
    "        List of teacher names for identification. Matching is\n",
    "        case-insensitive and accent-insensitive.\n",
    "    output_file : str, optional\n",
    "        If provided, write the merged CSV to this path.\n",
    "    manual_matches : dict, optional\n",
    "        Manual name mapping {moodle_name: eval_name} for edge cases.\n",
    "        These do not override automatic matches.\n",
    "    fuzzy_threshold : float, default=0.6\n",
    "        Minimum similarity score for automatic fuzzy matching.\n",
    "    suggestion_threshold : float, default=0.5\n",
    "        Minimum similarity for printing match suggestions to console.\n",
    "    salt : str, optional\n",
    "        Secret salt for pseudonym hashing. Strongly recommended.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    merged_df : pd.DataFrame\n",
    "        Merged and pseudonymized dataset.\n",
    "    pseudonym_mapping : dict\n",
    "        Mapping from original names to pseudonyms (for verification only).\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    The pseudonym_mapping should be kept confidential and not saved to disk\n",
    "    to prevent re-identification of participants.\n",
    "    \"\"\"\n",
    "\n",
    "    log_mask = make_log_name_masker()\n",
    "    \n",
    "    # Default teacher list (customize for your institution)\n",
    "    if teachers is None:\n",
    "        teachers = [\n",
    "            \"Alicia García Holgado\",\n",
    "            \"Andrea Vázquez Ingelmo\",\n",
    "            \"Francisco José García Peñalvo\"\n",
    "        ]\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 1: Load datasets\n",
    "    # =========================================================================\n",
    "    print(\"Loading datasets...\")\n",
    "    moodle_df = pd.read_csv(moodle_file)\n",
    "    eval_df = pd.read_csv(evaluation_file)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 2: Validate required columns\n",
    "    # =========================================================================\n",
    "    required_moodle = {'Nombre completo del usuario', 'Hora'}\n",
    "    required_eval = {'Nombre', 'Apellidos'}\n",
    "    \n",
    "    missing_cols = (\n",
    "        (required_moodle - set(moodle_df.columns)) |\n",
    "        (required_eval - set(eval_df.columns))\n",
    "    )\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {sorted(missing_cols)}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 3: Apply temporal filter\n",
    "    # =========================================================================\n",
    "    if limit_date:\n",
    "        limit_dt = datetime.strptime(limit_date, \"%d/%m/%Y\")\n",
    "        print(f\"Filtering Moodle actions after {limit_date} (inclusive end).\")\n",
    "        \n",
    "        moodle_df['__parsed_date'] = moodle_df['Hora'].apply(parse_date)\n",
    "        rows_before = len(moodle_df)\n",
    "        \n",
    "        moodle_df = moodle_df[\n",
    "            (moodle_df['__parsed_date'].isna()) |\n",
    "            (moodle_df['__parsed_date'] <= limit_dt)\n",
    "        ].drop(columns='__parsed_date')\n",
    "        \n",
    "        print(f\"Moodle rows kept: {len(moodle_df)} / {rows_before}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 4: Prepare evaluation data with normalized full names\n",
    "    # =========================================================================\n",
    "    eval_columns_expected = [\n",
    "        'Apellidos', 'Nombre',\n",
    "        'Hito 1', 'Hito 2', 'Hito 3', 'Trabajo',\n",
    "        'Coevaluación 1', 'Coevaluación 2', 'Coevaluación 3',\n",
    "        'Asistencia'\n",
    "    ]\n",
    "    present_eval_cols = [c for c in eval_columns_expected if c in eval_df.columns]\n",
    "    eval_subset = eval_df[present_eval_cols].copy()\n",
    "    \n",
    "    # Construct full name: \"FirstName LastName\" in uppercase\n",
    "    eval_subset['Nombre_completo'] = (\n",
    "        eval_df['Nombre'].astype(str).str.strip() + ' ' +\n",
    "        eval_df['Apellidos'].astype(str).str.strip()\n",
    "    ).str.upper().str.strip()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 5: Name matching (three phases)\n",
    "    # =========================================================================\n",
    "    moodle_names = [\n",
    "        n for n in moodle_df['Nombre completo del usuario'].dropna().unique()\n",
    "        if not is_system_account(n)\n",
    "    ]\n",
    "    eval_names = eval_subset['Nombre_completo'].dropna().unique().tolist()\n",
    "    \n",
    "    print(f\"Unique names in Moodle: {len(moodle_names)}\")\n",
    "    print(f\"Unique names in Evaluation: {len(eval_names)}\")\n",
    "    \n",
    "    name_mapping = {}\n",
    "    \n",
    "    # Phase 5a: Exact matches (O(N+M) via dictionary lookup)\n",
    "    eval_clean_to_orig = {clean_name(en): en for en in eval_names}\n",
    "    for mname in moodle_names:\n",
    "        mc = clean_name(mname)\n",
    "        if mc and mc in eval_clean_to_orig:\n",
    "            name_mapping[mname] = eval_clean_to_orig[mc]\n",
    "    \n",
    "    print(f\"Exact matches found: {len(name_mapping)}\")\n",
    "    \n",
    "    # Phase 5b: Fuzzy matches for remaining names\n",
    "    unmatched_moodle = [n for n in moodle_names if n not in name_mapping]\n",
    "    matched_eval = set(name_mapping.values())\n",
    "    unmatched_eval = [n for n in eval_names if n not in matched_eval]\n",
    "    \n",
    "    print(\"Finding fuzzy matches...\")\n",
    "    for mname in unmatched_moodle:\n",
    "        best, score = find_best_match(mname, unmatched_eval, threshold=fuzzy_threshold)\n",
    "        if best:\n",
    "            name_mapping[mname] = best\n",
    "            unmatched_eval.remove(best)  # Prevent duplicate matches\n",
    "            print(\n",
    "                \"Fuzzy match: \"\n",
    "                f\"'{log_mask(mname)}' -> '{log_mask(best)}' \"\n",
    "                f\"(score: {score:.3f})\"\n",
    "            )\n",
    "    \n",
    "    print(f\"Total matches found after fuzzy: {len(name_mapping)}\")\n",
    "    \n",
    "    # Phase 5c: Apply manual matches (do not override automatic matches)\n",
    "    if manual_matches:\n",
    "        print(\"Applying manual matches...\")\n",
    "        for mname, ename in manual_matches.items():\n",
    "            if mname in moodle_names and ename in eval_names:\n",
    "                if mname not in name_mapping:\n",
    "                    name_mapping[mname] = ename\n",
    "                    print(f\"Manual match: '{log_mask(mname)}' -> '{log_mask(ename)}'\")\n",
    "                else:\n",
    "                    print(f\"Manual match skipped (already matched): '{log_mask(mname)}'\")\n",
    "            else:\n",
    "                print(f\"Manual match ignored (name not found): '{log_mask(mname)}' -> '{log_mask(ename)}'\")\n",
    "    \n",
    "    # Report unmatched names and suggest potential matches\n",
    "    final_unmatched_moodle = [n for n in moodle_names if n not in name_mapping]\n",
    "    remaining_unmatched_eval = [n for n in eval_names if n not in name_mapping.values()]\n",
    "    \n",
    "    if final_unmatched_moodle:\n",
    "        masked = [log_mask(n) for n in final_unmatched_moodle]\n",
    "        print(\n",
    "            f\"\\nUnmatched in Moodle logs ({len(final_unmatched_moodle)}): \"\n",
    "            f\"{masked}\"\n",
    "        )\n",
    "    if remaining_unmatched_eval:\n",
    "        masked = [log_mask(n) for n in remaining_unmatched_eval]\n",
    "        print(\n",
    "            f\"\\nUnmatched in Moodle logs ({len(remaining_unmatched_eval)}): \"\n",
    "            f\"{masked}\"\n",
    "        )\n",
    "    \n",
    "    # Print suggestions for manual review (not automatically applied)\n",
    "    if final_unmatched_moodle and remaining_unmatched_eval:\n",
    "        print(f\"\\nSuggested matches (>={suggestion_threshold:.0%} similarity):\")\n",
    "        remaining_set = set(remaining_unmatched_eval)\n",
    "        for mname in final_unmatched_moodle:\n",
    "            best, best_score = None, 0.0\n",
    "            for ename in list(remaining_set):\n",
    "                score = calculate_name_similarity(mname, ename)\n",
    "                if score >= suggestion_threshold and score > best_score:\n",
    "                    best, best_score = ename, score\n",
    "            if best is not None:\n",
    "                print(\n",
    "                    f\"  '{log_mask(mname)}' <-> '{log_mask(best)}' \"\n",
    "                    f\"(score: {best_score:.3f})\"\n",
    "                )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 6: Filter Moodle rows (keep only teachers and matched students)\n",
    "    # =========================================================================\n",
    "    print(\"\\nFiltering unmatched non-teacher records...\")\n",
    "    print(f\"Original Moodle records: {len(moodle_df)}\")\n",
    "    \n",
    "    teachers_clean = {clean_name(t) for t in teachers}\n",
    "    filtered_counts = Counter()\n",
    "    \n",
    "    def keep_record(moodle_name):\n",
    "        if pd.isna(moodle_name):\n",
    "            filtered_counts[\"<NaN>\"] += 1\n",
    "            return False\n",
    "        if clean_name(moodle_name) in teachers_clean:\n",
    "            return True\n",
    "        if moodle_name in name_mapping:\n",
    "            return True\n",
    "        filtered_counts[moodle_name] += 1\n",
    "        return False\n",
    "    \n",
    "    moodle_df = moodle_df[\n",
    "        moodle_df['Nombre completo del usuario'].apply(keep_record)\n",
    "    ]\n",
    "    \n",
    "    # Mask names\n",
    "    masked_counts = {\n",
    "        (\"<NaN>\" if k == \"<NaN>\" else log_mask(k)): v\n",
    "        for k, v in filtered_counts.items()\n",
    "    }\n",
    "    \n",
    "    print(f\"Records removed per user (masked): {masked_counts}\")\n",
    "    print(f\"Filtered Moodle records: {len(moodle_df)}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 7: Merge evaluation fields onto Moodle records\n",
    "    # =========================================================================\n",
    "    print(\"\\nMerging datasets...\")\n",
    "    merged_rows = []\n",
    "    \n",
    "    eval_by_name = (\n",
    "        eval_subset.set_index('Nombre_completo')\n",
    "        if 'Nombre_completo' in eval_subset.columns\n",
    "        else pd.DataFrame()\n",
    "    )\n",
    "    \n",
    "    maybe_eval_cols = [\n",
    "        c for c in [\n",
    "            'Hito 1', 'Hito 2', 'Hito 3', 'Trabajo',\n",
    "            'Coevaluación 1', 'Coevaluación 2', 'Coevaluación 3', 'Asistencia'\n",
    "        ] if c in eval_subset.columns\n",
    "    ]\n",
    "    \n",
    "    for _, mrow in moodle_df.iterrows():\n",
    "        full_name = mrow['Nombre completo del usuario']\n",
    "        is_teacher = clean_name(full_name) in teachers_clean\n",
    "        \n",
    "        out = mrow.to_dict()\n",
    "        out['Es_profesor'] = bool(is_teacher)\n",
    "        \n",
    "        # Add evaluation data for matched students\n",
    "        if not is_teacher and full_name in name_mapping and not eval_by_name.empty:\n",
    "            eval_name = name_mapping[full_name]\n",
    "            if eval_name in eval_by_name.index:\n",
    "                row_eval = eval_by_name.loc[eval_name]\n",
    "                for c in maybe_eval_cols:\n",
    "                    out[c] = row_eval[c]\n",
    "            else:\n",
    "                for c in maybe_eval_cols:\n",
    "                    out[c] = pd.NA\n",
    "        else:\n",
    "            for c in maybe_eval_cols:\n",
    "                out[c] = pd.NA\n",
    "        \n",
    "        merged_rows.append(out)\n",
    "    \n",
    "    merged_df = pd.DataFrame(merged_rows)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 8: Pseudonymization\n",
    "    # =========================================================================\n",
    "    print(\"Pseudonymizing names...\")\n",
    "    original_names = merged_df['Nombre completo del usuario'].dropna().unique().tolist()\n",
    "    \n",
    "    # Assign deterministic teacher labels (sorted for reproducibility)\n",
    "    teacher_names_present = sorted(\n",
    "        [n for n in original_names if clean_name(n) in teachers_clean],\n",
    "        key=lambda x: clean_name(x)\n",
    "    )\n",
    "    \n",
    "    pseudonym_mapping = {}\n",
    "    for idx, tname in enumerate(teacher_names_present, start=1):\n",
    "        pseudonym_mapping[tname] = f\"TEACHER_{idx}\"\n",
    "    \n",
    "    # Generate salted hashes for students\n",
    "    for name in original_names:\n",
    "        if name not in pseudonym_mapping:\n",
    "            pseudonym_mapping[name] = create_pseudonym(name, salt=salt)\n",
    "    \n",
    "    merged_df['Nombre_pseudonimo'] = merged_df['Nombre completo del usuario'].map(\n",
    "        pseudonym_mapping\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 9: Compute attendance percentage and finalize\n",
    "    # =========================================================================\n",
    "    if 'Asistencia' in merged_df.columns:\n",
    "        merged_df['% Asistencia'] = merged_df['Asistencia'].apply(\n",
    "            lambda v: parse_attendance(v, total_sessions)\n",
    "        )\n",
    "    else:\n",
    "        merged_df['% Asistencia'] = pd.NA\n",
    "    \n",
    "    # Reorder columns: pseudonym and role first\n",
    "    leading_cols = ['Nombre_pseudonimo', 'Es_profesor']\n",
    "    other_cols = [c for c in merged_df.columns if c not in leading_cols]\n",
    "    merged_df = merged_df[leading_cols + other_cols]\n",
    "    \n",
    "    # Drop sensitive columns\n",
    "    sensitive_cols = [\n",
    "        'Nombre completo del usuario',\n",
    "        'Usuario afectado',\n",
    "        'Dirección IP',\n",
    "        'Origen'\n",
    "    ]\n",
    "    merged_df = merged_df.drop(\n",
    "        columns=[c for c in sensitive_cols if c in merged_df.columns],\n",
    "        errors='ignore'\n",
    "    )\n",
    "    \n",
    "    # Summary statistics\n",
    "    n_students = sum(v.startswith('STU_') for v in pseudonym_mapping.values())\n",
    "    n_teachers = sum(v.startswith('TEACHER_') for v in pseudonym_mapping.values())\n",
    "    \n",
    "    print(f\"Final merged dataset: {len(merged_df)} rows\")\n",
    "    print(f\"Students pseudonymized: {n_students}\")\n",
    "    print(f\"Teachers identified: {n_teachers}\")\n",
    "    \n",
    "    # Save output if path provided\n",
    "    if output_file:\n",
    "        merged_df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved to: {output_file}\")\n",
    "    \n",
    "    return merged_df, pseudonym_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dc4f46-75ce-4be8-8a29-6bea4251b8f4",
   "metadata": {},
   "source": [
    "## 4. Batch Processing Configuration\n",
    "\n",
    "This section configures the batch processing of multiple academic years. The configuration includes:\n",
    "\n",
    "- **Years**: List of academic year suffixes to process\n",
    "- **Per-year parameters**: Cutoff dates and session counts\n",
    "- **File templates**: Naming patterns for input and output files\n",
    "- **Teacher roster**: Names of instructors (for labeling, not pseudonymization)\n",
    "- **Study salt**: Secret value for cryptographic hashing (kept confidential)\n",
    "\n",
    "### Important Privacy Note\n",
    "\n",
    "The `STUDY_SALT` variable should be replaced with a strong, unique secret value for your study. This salt:\n",
    "- Must be kept confidential\n",
    "- Should never be logged, printed, or saved to output files\n",
    "- Enables consistent pseudonyms across processing runs\n",
    "- Prevents dictionary-based re-identification attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7431665f-71c4-4f81-b2d2-62e4732b739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# BATCH CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# Academic years to process (suffix used in file names)\n",
    "YEARS = ['22', '23', '24', '25']\n",
    "\n",
    "# Per-year configuration: cutoff date and total session count\n",
    "YEAR_CONFIG = {\n",
    "    '22': {'limit': '22/05/2022', 'sessions': 32},\n",
    "    '23': {'limit': '15/05/2023', 'sessions': 30},\n",
    "    '24': {'limit': '23/05/2024', 'sessions': 37},\n",
    "    '25': {'limit': '20/05/2025', 'sessions': 30}\n",
    "}\n",
    "\n",
    "# File naming templates (adjust paths as needed for your directory structure)\n",
    "MOODLE_TEMPLATE = 'moodle_{year}.csv'\n",
    "EVAL_TEMPLATE = 'evaluacion_{year}.csv'\n",
    "OUTPUT_TEMPLATE = 'merged_data_20{year}.csv'\n",
    "\n",
    "# Teacher roster for identification (case and accent insensitive matching)\n",
    "TEACHERS = [\n",
    "    'Alicia García Holgado',\n",
    "    'Andrea Vázquez Ingelmo',\n",
    "    'Francisco José García Peñalvo'\n",
    "]\n",
    "\n",
    "\n",
    "STUDY_SALT = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73217ee-590a-4473-bdb0-76f6c836d77b",
   "metadata": {},
   "source": [
    "## 5. Batch Execution\n",
    "\n",
    "The following cell executes the merge function for each academic year. For each year it:\n",
    "\n",
    "1. Verifies that input files exist\n",
    "2. Calls the merge function with year-specific parameters\n",
    "3. Adds a `Year` column for provenance tracking\n",
    "4. Stores results in memory for aggregation\n",
    "5. Writes anonymized per-year CSV files\n",
    "\n",
    "If a year's files are missing or processing fails, the error is logged and execution continues with the next year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ce45d9c-ec73-4a7b-8c6d-95543a8e3bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing academic year 2022\n",
      "============================================================\n",
      "Loading datasets...\n",
      "Filtering Moodle actions after 22/05/2022 (inclusive end).\n",
      "Moodle rows kept: 44535 / 54771\n",
      "Unique names in Moodle: 75\n",
      "Unique names in Evaluation: 72\n",
      "Exact matches found: 69\n",
      "Finding fuzzy matches...\n",
      "Fuzzy match: 'ALIAS_1' -> 'ALIAS_2' (score: 0.935)\n",
      "Fuzzy match: 'ALIAS_3' -> 'ALIAS_4' (score: 0.957)\n",
      "Total matches found after fuzzy: 71\n",
      "\n",
      "Unmatched in Moodle logs (4): ['ALIAS_5', 'ALIAS_6', 'ALIAS_7', 'ALIAS_8']\n",
      "\n",
      "Unmatched in Moodle logs (1): ['ALIAS_9']\n",
      "\n",
      "Suggested matches (>=50% similarity):\n",
      "\n",
      "Filtering unmatched non-teacher records...\n",
      "Original Moodle records: 44535\n",
      "Records removed per user (masked): {'ALIAS_10': 64, 'ALIAS_11': 505, 'ALIAS_8': 8}\n",
      "Filtered Moodle records: 43958\n",
      "\n",
      "Merging datasets...\n",
      "Pseudonymizing names...\n",
      "Final merged dataset: 43958 rows\n",
      "Students pseudonymized: 71\n",
      "Teachers identified: 3\n",
      "Saved to: merged_data_2022.csv\n",
      "\n",
      "[SUCCESS] Year 2022: 43958 rows -> merged_data_2022.csv\n",
      "\n",
      "============================================================\n",
      "Processing academic year 2023\n",
      "============================================================\n",
      "Loading datasets...\n",
      "Filtering Moodle actions after 15/05/2023 (inclusive end).\n",
      "Moodle rows kept: 52811 / 67841\n",
      "Unique names in Moodle: 75\n",
      "Unique names in Evaluation: 71\n",
      "Exact matches found: 71\n",
      "Finding fuzzy matches...\n",
      "Total matches found after fuzzy: 71\n",
      "\n",
      "Unmatched in Moodle logs (4): ['ALIAS_1', 'ALIAS_2', 'ALIAS_3', 'ALIAS_4']\n",
      "\n",
      "Filtering unmatched non-teacher records...\n",
      "Original Moodle records: 52811\n",
      "Records removed per user (masked): {'ALIAS_5': 1909, 'ALIAS_3': 36, 'ALIAS_4': 2, 'ALIAS_6': 24}\n",
      "Filtered Moodle records: 50840\n",
      "\n",
      "Merging datasets...\n",
      "Pseudonymizing names...\n",
      "Final merged dataset: 50840 rows\n",
      "Students pseudonymized: 71\n",
      "Teachers identified: 2\n",
      "Saved to: merged_data_2023.csv\n",
      "\n",
      "[SUCCESS] Year 2023: 50840 rows -> merged_data_2023.csv\n",
      "\n",
      "============================================================\n",
      "Processing academic year 2024\n",
      "============================================================\n",
      "Loading datasets...\n",
      "Filtering Moodle actions after 23/05/2024 (inclusive end).\n",
      "Moodle rows kept: 57321 / 57321\n",
      "Unique names in Moodle: 65\n",
      "Unique names in Evaluation: 65\n",
      "Exact matches found: 62\n",
      "Finding fuzzy matches...\n",
      "Fuzzy match: 'ALIAS_1' -> 'ALIAS_2' (score: 0.635)\n",
      "Total matches found after fuzzy: 63\n",
      "\n",
      "Unmatched in Moodle logs (2): ['ALIAS_3', 'ALIAS_4']\n",
      "\n",
      "Unmatched in Moodle logs (2): ['ALIAS_5', 'ALIAS_6']\n",
      "\n",
      "Suggested matches (>=50% similarity):\n",
      "\n",
      "Filtering unmatched non-teacher records...\n",
      "Original Moodle records: 57321\n",
      "Records removed per user (masked): {'ALIAS_7': 3398, 'ALIAS_8': 1059}\n",
      "Filtered Moodle records: 52864\n",
      "\n",
      "Merging datasets...\n",
      "Pseudonymizing names...\n",
      "Final merged dataset: 52864 rows\n",
      "Students pseudonymized: 63\n",
      "Teachers identified: 2\n",
      "Saved to: merged_data_2024.csv\n",
      "\n",
      "[SUCCESS] Year 2024: 52864 rows -> merged_data_2024.csv\n",
      "\n",
      "============================================================\n",
      "Processing academic year 2025\n",
      "============================================================\n",
      "Loading datasets...\n",
      "Filtering Moodle actions after 20/05/2025 (inclusive end).\n",
      "Moodle rows kept: 72539 / 72539\n",
      "Unique names in Moodle: 81\n",
      "Unique names in Evaluation: 79\n",
      "Exact matches found: 79\n",
      "Finding fuzzy matches...\n",
      "Total matches found after fuzzy: 79\n",
      "\n",
      "Unmatched in Moodle logs (2): ['ALIAS_1', 'ALIAS_2']\n",
      "\n",
      "Filtering unmatched non-teacher records...\n",
      "Original Moodle records: 72539\n",
      "Records removed per user (masked): {'ALIAS_3': 4259, 'ALIAS_4': 1212}\n",
      "Filtered Moodle records: 67068\n",
      "\n",
      "Merging datasets...\n",
      "Pseudonymizing names...\n",
      "Final merged dataset: 67068 rows\n",
      "Students pseudonymized: 79\n",
      "Teachers identified: 2\n",
      "Saved to: merged_data_2025.csv\n",
      "\n",
      "[SUCCESS] Year 2025: 67068 rows -> merged_data_2025.csv\n"
     ]
    }
   ],
   "source": [
    "# Storage for results across all years\n",
    "all_merged_data = {}\n",
    "all_name_mappings = {}\n",
    "\n",
    "for year_suffix in YEARS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing academic year 20{year_suffix}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Construct file paths\n",
    "    moodle_file = Path(MOODLE_TEMPLATE.format(year=year_suffix))\n",
    "    evaluation_file = Path(EVAL_TEMPLATE.format(year=year_suffix))\n",
    "    output_file = OUTPUT_TEMPLATE.format(year=year_suffix)\n",
    "    \n",
    "    # Validate input files exist\n",
    "    if not moodle_file.exists():\n",
    "        print(f\"[SKIP] Missing Moodle file: {moodle_file}\")\n",
    "        continue\n",
    "    if not evaluation_file.exists():\n",
    "        print(f\"[SKIP] Missing Evaluation file: {evaluation_file}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Execute the merge pipeline\n",
    "        merged_df, name_mapping = merge_moodle_evaluation_data(\n",
    "            moodle_file=str(moodle_file),\n",
    "            evaluation_file=str(evaluation_file),\n",
    "            total_sessions=YEAR_CONFIG[year_suffix]['sessions'],\n",
    "            limit_date=YEAR_CONFIG[year_suffix]['limit'],\n",
    "            teachers=TEACHERS,\n",
    "            output_file=str(output_file),\n",
    "            manual_matches=None,  # Add manual overrides here if needed\n",
    "            fuzzy_threshold=0.60,\n",
    "            suggestion_threshold=0.50,\n",
    "            salt=STUDY_SALT\n",
    "        )\n",
    "        \n",
    "        # Add provenance column for multi-year analyses\n",
    "        merged_df['Year'] = f\"20{year_suffix}\"\n",
    "        \n",
    "        # Store results\n",
    "        all_merged_data[f\"20{year_suffix}\"] = merged_df\n",
    "        all_name_mappings[f\"20{year_suffix}\"] = name_mapping\n",
    "        \n",
    "        print(f\"\\n[SUCCESS] Year 20{year_suffix}: {len(merged_df)} rows -> {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Year 20{year_suffix} failed: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combine-section",
   "metadata": {},
   "source": [
    "## 6. Combine All Years\n",
    "\n",
    "This cell concatenates all per-year datasets into a single combined file for longitudinal analyses. The `Year` column enables filtering by academic year as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d47ed0ea-9cb0-47af-8feb-0dfa21edeea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (214730, 17)\n",
      "Saved combined CSV to: merged_data_all_years.csv\n"
     ]
    }
   ],
   "source": [
    "if all_merged_data:\n",
    "    combined_df = pd.concat(all_merged_data.values(), ignore_index=True, sort=False)\n",
    "    combined_path = 'merged_data_all_years.csv'\n",
    "    combined_df.to_csv(combined_path, index=False)\n",
    "    \n",
    "    print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "    print(f\"Saved combined CSV to: {combined_path}\")\n",
    "else:\n",
    "    combined_df = pd.DataFrame()\n",
    "    print(\"No yearly datasets were produced; combined dataset is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "## 7. Processing Summary\n",
    "\n",
    "The following table summarizes the pseudonymization results for each academic year, showing the number of students and teachers processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f30fa9b-38d2-4f4f-a5d9-5214e889eef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Students (pseudonymized)</th>\n",
       "      <th>Teachers (labeled)</th>\n",
       "      <th>Total mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025</td>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Students (pseudonymized)  Teachers (labeled)  Total mapped\n",
       "0  2022                        71                   3            74\n",
       "1  2023                        71                   2            73\n",
       "2  2024                        63                   2            65\n",
       "3  2025                        79                   2            81"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary_data = []\n",
    "for year, mapping in all_name_mappings.items():\n",
    "    n_total = len(mapping)\n",
    "    n_teachers = sum(1 for v in mapping.values() if str(v).startswith('TEACHER_'))\n",
    "    n_students = n_total - n_teachers\n",
    "    summary_data.append((year, n_students, n_teachers, n_total))\n",
    "\n",
    "summary_df = pd.DataFrame(\n",
    "    summary_data,\n",
    "    columns=['Year', 'Students (pseudonymized)', 'Teachers (labeled)', 'Total mapped']\n",
    ")\n",
    "\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be474781-b73f-4c16-8eb6-1f1e144adbb3",
   "metadata": {},
   "source": [
    "## 8. Reproducibility Information\n",
    "\n",
    "The following cell prints environment information for reproducibility documentation. This can be included in the methods section of publications to ensure the analysis is auditable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53995f4b-faef-4166-b0d1-d68ab66ffea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing timestamp: 2025-12-11T13:15:31\n",
      "pandas version: 2.3.3\n",
      "Python hashlib: SHA-256 (salted)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processing timestamp: {datetime.now().isoformat(timespec='seconds')}\")\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"Python hashlib: SHA-256 (salted)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appendix-section",
   "metadata": {},
   "source": [
    "## Appendix: Column Descriptions\n",
    "\n",
    "### Output Dataset Columns\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `Nombre_pseudonimo` | str | Anonymized identifier (STU_XXXXXXXX or TEACHER_N) |\n",
    "| `Es_profesor` | bool | True if the record belongs to an instructor |\n",
    "| `Hora` | str | Timestamp of the Moodle activity |\n",
    "| `Contexto del evento` | str | Moodle context (course, module) |\n",
    "| `Componente` | str | Moodle component generating the event |\n",
    "| `Nombre evento` | str | Type of activity performed |\n",
    "| `Descripción` | str | Detailed event description |\n",
    "| `Hito 1-3` | float | Milestone grades (if available) |\n",
    "| `Trabajo` | float | Project/assignment grade |\n",
    "| `Coevaluación 1-3` | float | Peer evaluation scores |\n",
    "| `% Asistencia` | float | Attendance percentage [0-100] |\n",
    "| `Year` | str | Academic year (e.g., \"2022\") |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923dcc73-acd9-47e5-8be1-12e1ae848b38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
